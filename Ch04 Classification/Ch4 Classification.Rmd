Logistic regression
========================================================
```{r}
require(ISLR)
names(Smarket)
summary(Smarket)
#?Smarket
pairs(Smarket,col=Smarket$Direction)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data=Smarket,family=binomial)
summary(glm.fit)
#access just the coefficients for this fitted model.
coef(glm.fit)
summary(glm.fit)$coef
#
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
attach(Smarket)
contrasts(Direction)
table(glm.pred,Direction)
#to compute the fraction of days for which the prediction was correct.
mean(glm.pred==Direction)
```

In this case, logistic regression
correctly predicted the movement of the market 52.2% of the time.
At first glance, it appears that the logistic regression model is working
a little better than random guessing. However, this result is misleading
because we trained and tested the model on the same set of 1, 250 observations.
In other words, 100− 52.2 = 47.8% is the training error rate.

Make training and test set
The elements of the vector that correspond to
observations that occurred before 2005 are set to TRUE, whereas those that
correspond to observations in 2005 are set to FALSE.
```{r}
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Fit smaller model
glm.fit=glm(Direction~Lag1+Lag2,
            data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
106/(76+106)
```

In particular, we want to predict Direction on a
day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when
they equal 1.5 and −0.8.
```{r}
predict (glm.fit ,newdata =data.frame(Lag1=c(1.2 ,1.5) ,
                                      Lag2=c(1.1 , -0.8) ),type ="response")
```

Linear Discriminant Analysis LDA
========================================================
```{r}
require(MASS)
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
```
The coefficients of linear discriminants output provides the linear
combination of Lag1 and Lag2 that are used to form the LDA decision rule.
If −0.642×Lag1−0.514×Lag2 is large, then the LDA classifier will
predict a market increase, and if it is small, then the LDA classifier will
predict a market decline.
```{r}
lda.fit
```
The plot() function produces plots of the linear
discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for
each of the training observations.
```{r}
plot(lda.fit)
Smarket.2005=subset(Smarket,Year==2005)
lda.pred=predict(lda.fit,Smarket.2005)
lda.pred[1:5,]
class(lda.pred)
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)

sum(lda.pred$posterior [ ,1] >=.5)
sum(lda.pred$posterior [,1]<.5)

lda.pred$posterior [1:20 ,1]
```

Quadratic Discriminant Analysis
========================================================
```{r}
qda.fit=qda(Direction~Lag1+Lag2, data=Smarket ,subset =train)
qda.fit

qda.class =predict(qda.fit ,Smarket.2005) $class
table(qda.class ,Direction.2005)

mean(qda.class == Direction.2005)
```
Interestingly, the QDA predictions are accurate almost 60% of the time,
even though the 2005 data was not used to fit the model. This level of accuracy
is quite impressive for stock market data, which is known to be quite
hard to model accurately. This suggests that the quadratic form assumed
by QDA may capture the true relationship more accurately than the linear
forms assumed by LDA and logistic regression.

K-Nearest Neighbors
========================================================
```{r}
library(class)
#?knn
attach(Smarket)
Xlag=cbind(Lag1,Lag2)
train=Year<2005
knn.pred=knn(Xlag[train,],Xlag[!train,],Direction[train],k=1)
table(knn.pred,Direction[!train])
mean(knn.pred==Direction[!train])
```
An Application to Caravan Insurance Data
This data set includes 85 predictors that measure
demographic characteristics for 5,822 individuals. The response variable is
Purchase, which indicates whether or not a given individual purchases a
caravan insurance policy. In this data set, only 6% of people purchased
caravan insurance.
```{r}
#?Caravan
dim(Caravan)
#summary(Caravan)
attach(Caravan )

summary(Purchase )
```

**KNN classifier  - the scale of the variables matters !**
to handle this problem is to standardize the data so that all standardize
variables are given a mean of zero and a standard deviation of one. Then
all variables will be on a comparable scale.
```{r}
standardized.X=scale(Caravan [,-86])
var(Caravan [,1])
var( standardized.X[,1])

var(Caravan [,2])
var( standardized.X[,2])

test =1:1000
train.X=standardized.X[-test ,]
test.X=standardized.X[test ,]
train.Y=Purchase [-test]
test.Y=Purchase [test]
set.seed (1)
knn.pred=knn(train.X,test.X,train.Y,k=1)
mean(test.Y!= knn.pred)
mean(test.Y!="No")
```

Suppose that there is some non-trivial cost to trying to sell insurance
to a given individual. For instance, perhaps a salesperson must visit each
potential customer. If the company tries to sell insurance to a random
selection of customers, then the success rate will be only 6%, which may
be far too low given the costs involved. Instead, the company would like
to try to sell insurance only to customers who are likely to buy it. So the
overall error rate is not of interest. Instead, the fraction of individuals that
are correctly predicted to buy insurance is of interest.
It turns out that KNN with K = 1 does far better than random guessing
among the customers that are predicted to buy insurance. Among 77 such
customers, 9, or 11.7%, actually do purchase insurance. This is double the
rate that one would obtain from random guessing.
```{r}
table(knn.pred ,test.Y)
#9/(68+9) = 0.117

knn.pred=knn(train.X,test.X,train.Y,k=3)
table(knn.pred ,test.Y)
#5/26 = 0.192

knn.pred=knn (train.X,test.X,train.Y,k=5)
table(knn.pred ,test.Y)
#4/15 = 0.267

glm.fit=glm(Purchase~.,data=Caravan ,family=binomial, subset=-test)
glm.probs =predict(glm.fit ,Caravan [test ,], type="response")
glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.5]="Yes"
table(glm.pred,test.Y)

glm.pred=rep ("No" ,1000)
glm.pred[glm.probs >.25]="Yes"
table(glm.pred ,test.Y)
#11/(22+11) = 0.333
```

As a comparison, we can also fit a logistic regression model to the data.
If we use 0.5 as the predicted probability cut-off for the classifier, then
we have a problem: only seven of the test observations are predicted to
purchase insurance. Even worse, we are wrong about all of these! However,
we are not required to use a cut-off of 0.5. If we instead predict a purchase
any time the predicted probability of purchase exceeds 0.25, we get much
better results: we predict that 33 people will purchase insurance, and we
are correct for about 33% of these people. This is over five times better
than random guessing!