Leave-One-Out Cross-Validation (LOOCV)
========================================================
```{r}
require(ISLR)
require(boot)
#?cv.glm
plot(mpg~horsepower,data=Auto)

  
## LOOCV
glm.fit=glm(mpg~horsepower, data=Auto)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)

##Lets write a simple function to use formula (5.2)
loocv=function(fit){
  h=lm.influence(fit)$h
  mean((residuals(fit)/(1-h))^2)
}

## Now we try it out
loocv(glm.fit)
```

The cv.glm() function produces a list with several components. The two
numbers in the delta vector contain the cross-validation results.

We can repeat this procedure for increasingly complex polynomial fits.
To automate the process, we use the for() function to initiate a for loop

```{r}
cv.error=rep(0,5)
degree=1:5
for(d in degree){
  glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
  cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
```

k-Fold Cross-Validation
========================================================

```{r}
## 10-fold CV

cv.error10=rep(0,5)
for(d in degree){
  glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
  cv.error10[d]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
lines(degree,cv.error10,type="b",col="red")
```

Notice that the computation time is **much shorter** than that of LOOCV.
(In principle, the computation time for LOOCV for a least squares linear
model should be faster than for k-fold CV, due to the availability of the
formula (5.2) for LOOCV; however, unfortunately the cv.glm() function
does not make use of this formula.)

Bootstrap
========================================================
```{r}
## Minimum risk investment - Section 5.2

alpha=function(x,y){
  vx=var(x)
  vy=var(y)
  cxy=cov(x,y)
  (vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)

## What is the standard error of alpha?

alpha.fn=function(data, index){
  with(data[index,],alpha(X,Y))
}

alpha.fn(Portfolio,1:100)

set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))

boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
```

```{r}
setwd("C:/Users/mikhail/2014_02_StatLearning_R/Ch05 Resampling")
getwd()
load("5.R.RData")

summary(lm(y~X1+X2 ,data=Xy))$coef

boot.fn=function (data ,index )
return (coef(lm(y~X1+X2 ,data=Xy ,subset =index)))

boot.fn(Xy ,1:dim(Xy)[1])

boot(Xy ,boot.fn ,1000)

matplot(Xy,type="l")

boot.fn1=function (data )
return (coef(lm(y~X1+X2 ,data=Xy )))

tsboot(Xy ,boot.fn1 ,R = 100, l=1, sim = "fixed")

```
