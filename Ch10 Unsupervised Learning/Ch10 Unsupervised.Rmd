
Principal Components
====================
We will use the `USArrests` data (which is in R)
```{r}
dimnames(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2, var)
```
 
 We see that `Assault` has a much larger variance than the other variables. It would dominate the principal components, so we choose to standardize the variables when we perform PCA

 The **center** and **scale** components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.
 The rotation matrix provides the principal component loadings; each column
of pr.out$rotation contains the corresponding principal component
loading vector. 

```{r}
pca.out=prcomp(USArrests, scale=TRUE)
pca.out
names(pca.out)
```

The scale=0 argument to biplot() ensures that the arrows are scaled to represent the loadings
```{r}
biplot(pca.out, scale=0)
```
 We see that there are four distinct principal components. This is to be
expected because there are in general min(n âˆ’ 1, p) informative principal
components in a data set with n observations and p variables. 
 
 
```{r}
pca.out$sdev
pca.var =pca.out$sdev ^2
pca.var
```
 To compute the proportion of variance explained by each principal component,
we simply divide the variance explained by each principal component
by the total variance explained by all four principal components:

```{r}
pve=pca.var/sum(pca.var )
pve
```

 We see that the first principal component explains 62.0% of the variance in the data, the next principal component explains 24.7% of the variance, and so forth.
 We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:
```{r}
plot(pve , xlab=" Principal Component ", ylab=" Proportion of Variance Explained ", ylim=c(0,1) ,type='b')
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="Cumulative Proportion of Variance Explained ", ylim=c(0,1) , type='b')
```

K-Means Clustering
==================
K-means works in any dimension, but is most fun to demonstrate in two, because we can plot pictures.
Lets make some data with clusters. We do this by shifting the means of the points around.
```{r}
set.seed(101)
x=matrix(rnorm(100*2),100,2)
xmean=matrix(rnorm(8,sd=4),4,2)
which=sample(1:4,100,replace=TRUE)
x=x+xmean[which,]
plot(x,col=which,pch=19)
```
We know the "true" cluster IDs, but we wont tell that to the `kmeans` algorithm.

```{r}
km.out=kmeans(x,4,nstart=15)
km.out
plot(x,col=km.out$cluster,cex=2,pch=1,lwd=2)
points(x,col=which,pch=19)
points(x,col=c(4,3,2,1)[which],pch=19)
```

 To run the kmeans() function in R with multiple initial cluster assignments, we use the nstart argument.
If a value of nstart greater than one is used, then K-means clustering will be performed using multiple random
assignments in Step 1 of Algorithm 10.1, and the kmeans() function will report only the best results.

```{r}
set.seed (3)
km.out=kmeans(x,4,nstart=1)
km.out$tot.withinss

km.out=kmeans(x,4,nstart=15)
km.out$tot.withinss
```
 Note that km.out$tot.withinss is the total within-cluster sum of squares, which we seek to minimize by performing K-means clustering

 We strongly recommend always running K-means clustering with a large value of nstart, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.

Hierarchical Clustering
=======================
We will use these same data and use hierarchical clustering

```{r}
hc.complete=hclust(dist(x),method="complete")
plot(hc.complete)
hc.single=hclust(dist(x),method="single")
plot(hc.single)
hc.average=hclust(dist(x),method="average")
plot(hc.average)

```
Lets compare this with the actualy clusters in the data. We will use the function `cutree` to cut the tree at level 4.
This will produce a vector of numbers from 1 to 4, saying which branch each observation is on. You will sometimes see pretty plots where the leaves of the dendrogram are colored. I searched a bit on the web for how to do this, and its a little too complicated for this demonstration.

We can use `table` to see how well they match:
```{r}
hc.cut=cutree(hc.complete,4)
table(hc.cut,which)
table(hc.cut,km.out$cluster)
```
or we can use our group membership as labels for the leaves of the dendrogram:
```{r}
plot(hc.complete,labels=which)
```
 
